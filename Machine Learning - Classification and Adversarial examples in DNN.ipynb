{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xznGYZwvbB-Q"
   },
   "source": [
    "##### This assignment is consists of two parts, i.e., Part 1: Classification with Deep Neural Networks & Part 2: Adversarial Examples for Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20YjLlXLbB-U"
   },
   "source": [
    "## Part 1: Classification Task with Deep Neural Networks\n",
    "\n",
    "In this part, you will implement two neural networks (one neural network and one convolution neural network) using PyTorch to classify images of Fashion MNIST dataset (https://research.zalando.com/project/fashion_mnist/fashion_mnist/). Fashion-MNIST is a dataset of Zalando’s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement of the original MNIST dataset (http://yann.lecun.com/exdb/mnist/) for benchmarking machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZdv-JA9iPoX"
   },
   "source": [
    "First, we need to import some useful libraries. If your computer have not install the pytorch, you are encouraged to finish the first part of the program via Colab (https://research.google.com/colaboratory/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ENCpJqOGhg-l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWT2lJqcspfm"
   },
   "source": [
    "Please first finish the following simple task to review some useful functions in classification:\n",
    "\n",
    "(a) **[5 points]** Please realize an util function: <em>one_hot_labels</em>.\n",
    "\n",
    "In the implementation of the cross entropy loss, it is much convenient if numerical labels are transformed into one-hot labels. For example, numerical label 5 -> one-hot label [0,0,0,0,0,1,0,0,0,0]. Accordingly, the cross entropy loss can be written as follow:\n",
    "\n",
    "$CE(y,\\hat y)=-\\sum_{k=1}^K y_k \\log \\hat y_k$,\n",
    "\n",
    "where $\\hat y$ is the softmax outputs from the model for the training example, $y$ is the one-hot (ground-truth) label, and the subscript refers to the element of $y$ at the coordinate $k$. \n",
    "\n",
    "(b) **[5 points]** Please realize the softmax function as well as the sigmoid function: <em>softmax(x)</em> and <em>sigmoid(x)</em>.\n",
    "\n",
    "The $k$-th element of softmax is calculated via:\n",
    "\n",
    "$softmax(x)_k=\\frac {e^{x_k}}{\\sum_j e^{x_j}}=\\frac {e^{x_k+c}}{\\sum_j e^{x_j+c}}$\n",
    "\n",
    "The last equation holds since adding a constant won't change softmax results. Note that, you may encounter an overflow when softmax computes the exponential, so please using the 'max' tricks to avoid this problem.  \n",
    "\n",
    "The sigmoid is calculated by:\n",
    "\n",
    "$sigmoid(x)=\\frac {1}{1+e^{-x}} = \\frac {e^x}{e^x + 1}$\n",
    "\n",
    "For numerical stability, please use the 1st equation for positive inputs, and the 2nd equation for negative inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "do9k-CvztDrk"
   },
   "outputs": [],
   "source": [
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    ### END YOUR CODE\n",
    "    return one_hot_labels\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x is of shape: batch_size * #class\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    c = np.max(x, axis=1, keepdims=True)\n",
    "    numerator = np.exp(x - c)\n",
    "    denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "    s = numerator / denominator\n",
    "    ### END YOUR CODE\n",
    "    return s\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    x is of shape: batch_size * dim_hidden\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "    \n",
    "    z = np.zeros_like(x, dtype=float)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])\n",
    "    \n",
    "    top = np.ones_like(x, dtype=float)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    s = top / (1 + z)\n",
    "    ### END YOUR CODE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFAiRnqig4l"
   },
   "source": [
    "Now, let's begin the classification task after you get some initial idea about the loss functions defined before.\n",
    "\n",
    "(c) **[5 points]** Please load the Fashion-MNIST by the datasets provided by torchvision. (https://pytorch.org/vision/stable/datasets.html)\n",
    "\n",
    "If you have not downloaded the FashionMNIST dataset, you can download it by setting appropriate parameters. As for the transform, you may feel free to chose transforms for your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "_1w3ChyBgPE2"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")   \n",
    "### END YOUR CODE\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=128)\n",
    "test_dataloader = DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3O7doHZjcG0"
   },
   "source": [
    "(d) **[10 points]** Please define** the first model class** (DeepNeuralNetwork: a simple neural network) as follows, the DeepNeuralNetwork should match both the input dimention for the images and output dimention for classification task. It is recommended that the model include (linear layer, relu layer, linear layer, relu layer, linear layer)-liked structure for the task (you are also encouraged to choose any advanced structure settings to achieve higher task accuracy), and you also need to define the corresponding forward function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "7tsBBmOgb4nN"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        ### END CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE HERE\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "        ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxbo5CGupZph"
   },
   "source": [
    "(e) **[12 points]** Please define **the second model class** (ConvNeuralNetwork: a simple convolution neural network) as follows, the ConvNeuralNetwork should also match both the input dimention for the images and output dimention for classification task. It is recommended that the model include two convolution layers (each layer should contain nn.Conv2d, nn.ReLU and nn.MaxPool2d https://pytorch.org/docs/stable/nn.html#convolution-layers )-liked structure for the task (you are also encouraged to choose any advanced structure settings to achieve higher task accuracy), and you also need to define the corresponding forward function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "phAXQdKtpbwY"
   },
   "outputs": [],
   "source": [
    "class ConvNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        ### YOUR CODE HERE\n",
    "        super(ConvNeuralNetwork, self).__init__()\n",
    "        # The first Conv\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1,6,kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2 ,stride = 2,padding=0)\n",
    "        )\n",
    "        # The second Conv\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6,16,kernel_size=5,stride=1,padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2 ,stride = 2,padding=0)            \n",
    "        )\n",
    "        # The output layer\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16*5*5,120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Linear(84,10)        \n",
    "        \n",
    "\n",
    "        ### END CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE HERE\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)    \n",
    "        return x\n",
    "        ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltVsBjaGm84t"
   },
   "source": [
    "(f) **[3 points]** Please define the learning_rate, batch_size, training_epochs (5) and loss_function (nn.CrossEntropyLoss) using torch.nn. You may feel free to choose the training parameters but please try your best to find a better parameter which can well train your defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "3kuwnRdPgTur"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "learning_rate = 1e-3 #(I will compare two leraning rate; 1e-3 and 2e-1) )\n",
    "batch_size =512\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WhpOLxGnfdG"
   },
   "source": [
    "(g) **[5 points]** Please finish the training function which uses the previous defined model, loss function and optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "4ArQBII_cJyJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    ### YOUR CODE HERE\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    ### END CODE HERE\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wGY_4mwnuVU"
   },
   "source": [
    "(h) **[5 points]** Please finish the test function which uses the previous defined model and loss function. In the test function, you need to calculate the test accuracy for each epoch,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "-zkenNDTgY23"
   },
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    ### YOUR CODE HERE\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    ### END CODE HERE\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxVyGDfcoXoj"
   },
   "source": [
    "(i) **[5 points]** Please try the first deep neural network that defined before using the training and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52SBza-1ca_l",
    "outputId": "4d81a0f6-0605-41a9-afd1-ef367cc9e475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301959  [    0/60000]\n",
      "loss: 2.280067  [12800/60000]\n",
      "loss: 2.269731  [25600/60000]\n",
      "loss: 2.260156  [38400/60000]\n",
      "loss: 2.237974  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 19.3%, Avg loss: 2.228125 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.232590  [    0/60000]\n",
      "loss: 2.203154  [12800/60000]\n",
      "loss: 2.194361  [25600/60000]\n",
      "loss: 2.186452  [38400/60000]\n",
      "loss: 2.156604  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 2.143525 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.146974  [    0/60000]\n",
      "loss: 2.105623  [12800/60000]\n",
      "loss: 2.094529  [25600/60000]\n",
      "loss: 2.081418  [38400/60000]\n",
      "loss: 2.043523  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 2.023018 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.022669  [    0/60000]\n",
      "loss: 1.965381  [12800/60000]\n",
      "loss: 1.949336  [25600/60000]\n",
      "loss: 1.924864  [38400/60000]\n",
      "loss: 1.883783  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.853031 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.845171  [    0/60000]\n",
      "loss: 1.773854  [12800/60000]\n",
      "loss: 1.755872  [25600/60000]\n",
      "loss: 1.721545  [38400/60000]\n",
      "loss: 1.691498  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.654387 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#model = DeepNeuralNetwork()\n",
    "#lr = 1e-3\n",
    "model = NeuralNetwork()\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "### END CODE HERE\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305113  [    0/60000]\n",
      "loss: 0.595381  [12800/60000]\n",
      "loss: 0.535711  [25600/60000]\n",
      "loss: 0.463916  [38400/60000]\n",
      "loss: 0.619123  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.471545 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.344490  [    0/60000]\n",
      "loss: 0.434144  [12800/60000]\n",
      "loss: 0.365777  [25600/60000]\n",
      "loss: 0.373672  [38400/60000]\n",
      "loss: 0.500140  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.438777 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.304787  [    0/60000]\n",
      "loss: 0.365933  [12800/60000]\n",
      "loss: 0.319774  [25600/60000]\n",
      "loss: 0.334838  [38400/60000]\n",
      "loss: 0.443920  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.408814 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.271616  [    0/60000]\n",
      "loss: 0.325942  [12800/60000]\n",
      "loss: 0.296693  [25600/60000]\n",
      "loss: 0.306548  [38400/60000]\n",
      "loss: 0.416288  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.384554 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.243331  [    0/60000]\n",
      "loss: 0.271311  [12800/60000]\n",
      "loss: 0.286757  [25600/60000]\n",
      "loss: 0.294780  [38400/60000]\n",
      "loss: 0.376679  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.369588 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#learning_rate =2e-1\n",
    "model = NeuralNetwork()\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2e-1)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "### END CODE HERE\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so learning rate = 2e-1 has higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0ltPsw7uhRB"
   },
   "source": [
    "(j) **[5 points]** Please try the second deep neural network that defined before using the training and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydc09EdSokA_",
    "outputId": "eb8914d8-4eb8-4456-d739-5b18989d094b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306318  [    0/60000]\n",
      "loss: 2.300606  [12800/60000]\n",
      "loss: 2.300470  [25600/60000]\n",
      "loss: 2.306922  [38400/60000]\n",
      "loss: 2.293545  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.298968 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.301943  [    0/60000]\n",
      "loss: 2.296512  [12800/60000]\n",
      "loss: 2.295878  [25600/60000]\n",
      "loss: 2.302437  [38400/60000]\n",
      "loss: 2.289320  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.294353 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.297395  [    0/60000]\n",
      "loss: 2.291750  [12800/60000]\n",
      "loss: 2.290508  [25600/60000]\n",
      "loss: 2.297690  [38400/60000]\n",
      "loss: 2.283702  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.288509 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.291754  [    0/60000]\n",
      "loss: 2.285229  [12800/60000]\n",
      "loss: 2.283320  [25600/60000]\n",
      "loss: 2.291492  [38400/60000]\n",
      "loss: 2.275774  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.280158 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.283870  [    0/60000]\n",
      "loss: 2.275595  [12800/60000]\n",
      "loss: 2.272617  [25600/60000]\n",
      "loss: 2.281549  [38400/60000]\n",
      "loss: 2.262955  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.2%, Avg loss: 2.266246 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#learning rate = 1e-3\n",
    "model = ConvNeuralNetwork()\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "### YOUR CODE HERE\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "### END CODE HERE\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306721  [    0/60000]\n",
      "loss: 1.755281  [12800/60000]\n",
      "loss: 0.793896  [25600/60000]\n",
      "loss: 0.545092  [38400/60000]\n",
      "loss: 0.554672  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.509154 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.411477  [    0/60000]\n",
      "loss: 0.399081  [12800/60000]\n",
      "loss: 0.394338  [25600/60000]\n",
      "loss: 0.421502  [38400/60000]\n",
      "loss: 0.489950  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.423665 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.297231  [    0/60000]\n",
      "loss: 0.354954  [12800/60000]\n",
      "loss: 0.330187  [25600/60000]\n",
      "loss: 0.364467  [38400/60000]\n",
      "loss: 0.452863  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.379614 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.240598  [    0/60000]\n",
      "loss: 0.325760  [12800/60000]\n",
      "loss: 0.285345  [25600/60000]\n",
      "loss: 0.340598  [38400/60000]\n",
      "loss: 0.414508  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.351869 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.209900  [    0/60000]\n",
      "loss: 0.316799  [12800/60000]\n",
      "loss: 0.255484  [25600/60000]\n",
      "loss: 0.317440  [38400/60000]\n",
      "loss: 0.376528  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.333702 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#learning rate = 2e-1\n",
    "model = ConvNeuralNetwork()\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2e-1)\n",
    "### YOUR CODE HERE\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "### END CODE HERE\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in general learning rate = 2e-1 has higher accuracy and less avg loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lsqDcgxbB-f"
   },
   "source": [
    "## Part 2: Adversarial Examples for Neural Networks\n",
    "\n",
    "It has been seen that many classifiers, including neural networks, are highly susceptible to what are called adversarial examples -- small perturbations of the input that cause a classifier to misclassify, but are imperceptible to humans. For example, making a small change to an image of a stop sign might cause an object detector in an autonomous vehicle to classify it as an yield sign, which could lead to an accident.\n",
    "\n",
    "In this part, we will see **how to construct adversarial examples for neural networks**, and you are given a 3-hidden layer perceptron trained on the MNIST dataset for this purpose.\n",
    "\n",
    "Since we are interested in constructing the countersample rather than the original classification task, we do not need to worry too much about the design of the neural network and the processing of the data (which are already given). The parameters of the perceptron can be loaded from fc\\*.weight,npy and fc\\*.bias.npy. The test dataset can be loaded from X_test.npy and Y_test.npy. Each image of MNIST is 28×28 pixels in size, and is generally represented as a flat vector of 784 numbers. It also includes labels for each example, a number indicating the actual digit (0 - 9) handwritten in that image. \n",
    "\n",
    "**Enjoy practicing generating adversarial examples and have fun!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ow4zOGE6bB-g"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIf5hpENbB-g"
   },
   "source": [
    "First, we need to define some functions for later computing.\n",
    "\n",
    "(a) **[5 points]** Please realize the following functions: \n",
    "\n",
    "relu: The relu function is calculated as:\n",
    "\n",
    "$$\n",
    "relu(x)=max(0,x)\n",
    "$$\n",
    "\n",
    "relu_grad: The relu_grad is used to compute the gradient of relu function as:\n",
    "\n",
    "$$\n",
    "relu\\_grad(x)=(1(x>0),0(x \\leq 0))\n",
    "$$\n",
    "\n",
    "one_hot: In the implementation of the cross entropy loss, it is much convenient if numerical labels are transformed into one-hot labels. For example, numerical label 6 -> one-hot label [0,0,0,0,0,0,1,0,0,0]. Accordingly, the cross entropy loss can be written as follow:\n",
    "\n",
    "$$\n",
    "cross\\_entropy(y,\\hat y)=-\\sum_{k=1}^K y_k \\log \\hat y_k\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ is the softmax outputs from the model for the training example, $y$ is the one-hot (ground-truth) label, and the subscript refers to the element of $y$ at the coordinate $k$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "byVQjSzLbB-g"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Input\n",
    "        x: a vector in ndarray format\n",
    "    Output\n",
    "        relu_x: a vector in ndarray format,\n",
    "        representing the ReLu activation of x.\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    relu_x = np.maximum(x, 0)\n",
    "    ### END YOUR CODE\n",
    "    return relu_x\n",
    "\n",
    "def relu_grad(x):\n",
    "    '''\n",
    "    Input\n",
    "        x: a vector in ndarray format\n",
    "    Output\n",
    "        relu_grad_x: a vector in ndarray format,\n",
    "        representing the gradient of ReLu activation.\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "     \n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    relu_grad_x = x\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    return relu_grad_x\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    '''\n",
    "    Input\n",
    "        y: an int representing the class label\n",
    "        y_hat: a vector in ndarray format showing the predicted\n",
    "           probability of each class.\n",
    "           \n",
    "    Output\n",
    "        the cross entropy loss. \n",
    "    '''\n",
    "    log_likelihood = -np.log(y_hat.squeeze())\n",
    "    return log_likelihood[y]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbPpJ7CubB-h"
   },
   "source": [
    "Next, we define the structure and some utility functions of our multi-layer perceptron.\n",
    "\n",
    "\n",
    "The neural net is a fully-connected multi-layer perceptron with three hidden layers. The hidden layers contains 2048, 512 and 512 hidden nodes respectively. We use ReLU as the activation function at each hidden node. The last intermediate layer’s output is passed through a softmax function, and the loss is measured as the cross-entropy between the resulted probability vector and the true label.\n",
    "\n",
    "\n",
    "    x: the input image vector with dimension 1x784.\n",
    "    y: the true class label of x.\n",
    "    zi: the value of the i-th intermediate layer before activation, with dimension 1x2048, 1x512, 1x512 and 1x10 for i = 1, 2, 3, 4.\n",
    "    hi: the value of the i-th intermediate layer after activation, with dimension 1x2048, 1x512 and 1x512 for i = 1, 2, 3.\n",
    "    p: the predicted class probability vector after the softmax function, with dimension 1x10.\n",
    "    Wi: the weights between the (i - 1)-th and the i-th intermediate layer. For simplicity, we use h0 as an alias to x. Each Wi has dimension li_1 x li, where li is the number of nodes in the i-th layer. For example, W1 ha dimension 784x2048.\n",
    "    bi: the bias between the (i - 1)-th and the i-th intermediate layer. The dimension is 1 x li.\n",
    "\n",
    "(b) **[20 points]** Please realize the forward propogation and the gradient calculation:\n",
    "\n",
    "**[10 points]** The forward propagation rules are as follows.\n",
    "$$\n",
    "z^i = h^{i-1} \\cdot W^i + b^i, i = 1, 2, 3, 4.\n",
    "$$\n",
    "\n",
    "$$\n",
    "h^i = ReLU(z^i), i = 1, 2, 3.\n",
    "$$\n",
    "\n",
    "$$\n",
    "p = Softmax(z^4)\n",
    "$$\n",
    "\n",
    "**[10 points]** The gradient calculation rules are as follows.\n",
    "\n",
    "Let L denote the cross entropy loss of an image-label pair (x, y). We are interested in the gradient of L w.r.t. x, and move x in the direction of (the sign of) the gradient to increase L. If L becomes large, the new image x_adv will likely be misclassified.\n",
    "\n",
    "We use chain rule for gradient computation. Again, let h0 be the alias of x. We have:\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta x} =  \\frac{\\delta L}{\\delta z^4} \\frac{\\delta z^4}{\\delta h^3} \\prod_{i=1}^{3} \\{ \\frac{\\delta h^i}{\\delta z^i} \\frac{\\delta z^i}{\\delta h^{i-1}} \\}\n",
    "$$\n",
    "\n",
    "The intermediate terms can be computed as follows.\n",
    "\n",
    "$$\n",
    "\\frac{\\delta L}{z^4} = p - one\\_hot(y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{z^i}{h^{i-1}} = (W^i)^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\delta h^i}{\\delta z^i}=relu\\_grad(z^i)\n",
    "$$\n",
    "\n",
    "(c) **[10 points]** Please generate the adversarial examples based on the gradient.\n",
    "\n",
    "We begin with deriving a simple way of constructing an adversarial example around an input (x, y).\n",
    "Supppose we denote our neural network by a function f: X $\\rightarrow$ {0,...,9}.\n",
    "Suppose we want to find a small perturbation $\\Delta$ of x such that the neural network f assigns a label different from y to x+$\\Delta$. To find such a $\\Delta$, we want to increase the cross-entropy loss of the network f at (x, y); in other words, we want to take a small step $\\Delta$ along which the cross-entropy loss increases, thus causing a misclassification. We can write this as a gradient ascent update, and to ensure that we only take a small step, we can just use the sign of each coordinate of the gradient. The final algorithm is this:\n",
    "\n",
    "$$\n",
    "x_{adv} = x + \\epsilon \\cdot sign (\\nabla L(f(x), y))\n",
    "$$\n",
    "\n",
    "where L is the cross-entropy loss, and it is known as the **Fast Gradient Sign Method (FGSM)**. In this time, you are required to realize the multi-times FGSM, which is known to be **PGD** method. To be specific, for overall $n$ times, we will conduct: \n",
    "\n",
    "$$\n",
    "x_{adv}^t = x + (\\epsilon / n) \\cdot sign (\\nabla L(f(x_{adv}^{t-1}), y))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "3BrkcPgWbB-h"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    '''\n",
    "    This class defines the multi-layer perceptron we will be using\n",
    "    as the attack target.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.eps = 0.1\n",
    "    \n",
    "    def load_params(self, params):\n",
    "        '''\n",
    "        This method loads the weights and biases of a trained model.\n",
    "        '''\n",
    "        self.W1 = params[\"fc1.weight\"]\n",
    "        self.b1 = params[\"fc1.bias\"]\n",
    "        self.W2 = params[\"fc2.weight\"]\n",
    "        self.b2 = params[\"fc2.bias\"]\n",
    "        self.W3 = params[\"fc3.weight\"]\n",
    "        self.b3 = params[\"fc3.bias\"]\n",
    "        self.W4 = params[\"fc4.weight\"]\n",
    "        self.b4 = params[\"fc4.bias\"]\n",
    "        \n",
    "    def set_attack_budget(self, eps):\n",
    "        '''\n",
    "        This method sets the maximum L_infty norm of the adversarial\n",
    "        perturbation.\n",
    "        '''\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This method finds the predicted probability vector of an input\n",
    "        image x.\n",
    "        \n",
    "        Input\n",
    "            x: a single image vector in ndarray format\n",
    "        Ouput\n",
    "            self.p: a vector in ndarray format representing the predicted class\n",
    "            probability of x.\n",
    "            \n",
    "        Intermediate results are stored as class attributes.\n",
    "        You might need them for gradient computation.\n",
    "        '''\n",
    "        W1, W2, W3, W4 = self.W1, self.W2, self.W3, self.W4\n",
    "        b1, b2, b3, b4 = self.b1, self.b2, self.b3, self.b4\n",
    "        \n",
    "        self.z1 = np.matmul(x,W1)+b1\n",
    "        #######################################\n",
    "        ### YOUR CODE HERE\n",
    "        if len(self.z1.shape) < 2:\n",
    "            self.z1 = np.expand_dims(self.z1, axis=0)\n",
    "        self.h1 = relu(self.z1)\n",
    "        self.z2 = np.matmul(self.h1,W2)+b2\n",
    "        self.h2 = relu(self.z2)\n",
    "        self.z3 = np.matmul(self.h2,W3)+b3\n",
    "        self.h3 = relu(self.z3)\n",
    "        self.z4 = np.matmul(self.h3,W4)+b4\n",
    "        self.p = softmax(self.z4)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        #######################################\n",
    "\n",
    "        return self.p\n",
    "        \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        This method takes a single image vector x and returns the \n",
    "        predicted class label of it.\n",
    "        '''\n",
    "        res = self.forward(x)\n",
    "        return np.argmax(res)\n",
    "\n",
    "\n",
    "    def gradient(self,x,y):\n",
    "        ''' \n",
    "        This method finds the gradient of the cross-entropy loss\n",
    "        of an image-label pair (x,y) w.r.t. to the image x.\n",
    "        \n",
    "        Input\n",
    "            x: the input image vector in ndarray format\n",
    "            y: the true label of x\n",
    "            \n",
    "        Output\n",
    "            grad: a vector in ndarray format representing\n",
    "            the gradient of the cross-entropy loss of (x,y)\n",
    "            w.r.t. the image x.\n",
    "        '''\n",
    "        \n",
    "        #######################################\n",
    "        ### YOUR CODE HERE\n",
    "         ### YOUR CODE HERE\n",
    "        L = cross_entropy(y, self.forward(x))\n",
    "\n",
    "        delta_L_z4 = self.forward(x)-one_hot_labels(y)\n",
    "        delta_z4_h3 = self.W4.T\n",
    "        delta_h3_z3 = relu_grad(self.z3)\n",
    "        delta_z3_h2 = self.W3.T\n",
    "        delta_h2_z2 = relu_grad(self.z2)\n",
    "        delta_z2_h1 = self.W2.T\n",
    "        delta_h1_z1 = relu_grad(self.z1)\n",
    "        delta_z1_h0 = self.W1.T\n",
    "        inter1 = np.dot(delta_L_z4, delta_z4_h3)\n",
    "        inter2 = np.multiply(inter1, delta_h3_z3)\n",
    "        inter3 = np.dot(inter2, delta_z3_h2)\n",
    "        inter4 = np.multiply(inter3, delta_h2_z2)\n",
    "        inter5 = np.dot(inter4, delta_z2_h1)\n",
    "        inter6 = np.multiply(inter5, delta_h1_z1)\n",
    "        inter7 = np.dot(inter6, delta_z1_h0)\n",
    "        grad = inter7\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        #######################################\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def attack(self,x,y,n=2):\n",
    "        '''\n",
    "        This method generates the adversarial example of an\n",
    "        image-label pair (x,y).\n",
    "        \n",
    "        Input\n",
    "            x: an image vector in ndarray format, representing\n",
    "               the image to be corrupted.\n",
    "            y: the true label of the image x.\n",
    "            \n",
    "        Output\n",
    "            x_adv: a vector in ndarray format, representing\n",
    "            the adversarial example created from image x.\n",
    "        '''\n",
    "        \n",
    "        #######################################\n",
    "        ### YOUR CODE HERE\n",
    "        for i in range(n):\n",
    "            if(i==0):\n",
    "                z= np.sign(self.gradient(x, y))\n",
    "                x_new= x+ (self.eps/(n))*z\n",
    "            else:\n",
    "                z = np.sign(self.gradient(x_new, y))\n",
    "                x_adv = x + (self.eps/(n)) * z\n",
    "\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        #######################################\n",
    "        \n",
    "        return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KIvUJcLbB-h"
   },
   "source": [
    "Now, let's load the test data and the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "61riiaBYbB-h",
    "outputId": "42bbb1ca-3802-48a8-85c6-c6886c5885b6"
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"./X_test.npy\")\n",
    "Y_test = np.load(\"./Y_test.npy\")\n",
    "\n",
    "params = {}\n",
    "param_names = [\"fc1.weight\", \"fc1.bias\",\n",
    "               \"fc2.weight\", \"fc2.bias\",\n",
    "               \"fc3.weight\", \"fc3.bias\",\n",
    "               \"fc4.weight\", \"fc4.bias\"]\n",
    "\n",
    "for name in param_names:\n",
    "    params[name] = np.load(\"./\"+name+'.npy')\n",
    "    \n",
    "clf = MultiLayerPerceptron()\n",
    "clf.load_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0OSAWNnbB-i"
   },
   "source": [
    "Check if the image data are loaded correctly. Let's visualize the first image in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "PHPYcMCkbB-i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an image of Number 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25d8a4a2430>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMD0lEQVR4nO3dXagc5R3H8d+vabwwepFUE0OUxIqiRTEpQYSEavEFG4SYC4sRSqTC8cJAhF5U7IVCKUio9sIL4YjBVKwvRINR60sIkrQ3mqOmGo1GK6kec8hRFHxDrMm/F2dSjvHs7HFnZmc9/+8HDrs7z87OnyG/PM/szOzjiBCAme9HbRcAoD8IO5AEYQeSIOxAEoQdSOLH/dyYbb76BxoWEZ5qeaWe3fYVtt+y/Y7tm6t8FoBmudfz7LZnSdov6TJJo5J2S1obEW+UrEPPDjSsiZ79AknvRMS7EfG1pIckra7weQAaVCXsiyS9P+n1aLHsW2wP2R6xPVJhWwAqqvIF3VRDhe8M0yNiWNKwxDAeaFOVnn1U0mmTXp8q6WC1cgA0pUrYd0s60/bpto+TdI2kbfWUBaBuPQ/jI+Ib2+slPStplqRNEfF6bZUBqFXPp9562hjH7EDjGrmoBsAPB2EHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSfR1ymbkc9ZZZ3Vse/PNN0vX3bBhQ2n7XXfd1VNNWdGzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdHo5YtW9ax7ciRI6Xrjo6O1l1OapXCbvuApM8kHZb0TUQsr6MoAPWro2f/ZUR8VMPnAGgQx+xAElXDHpKes/2S7aGp3mB7yPaI7ZGK2wJQQdVh/IqIOGh7vqTttt+MiF2T3xARw5KGJcl2VNwegB5V6tkj4mDxOC5pq6QL6igKQP16DrvtObZPPPpc0uWS9tZVGIB6VRnGL5C01fbRz/lbRDxTS1WYMZYuXdqx7Ysvvihdd+vWrTVXk1vPYY+IdyWdX2MtABrEqTcgCcIOJEHYgSQIO5AEYQeS4BZXVHLuueeWtq9fv75j2/333193OShBzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXCeHZWcffbZpe1z5szp2Pbwww/XXQ5K0LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKO6N8kLcwIM/O8+OKLpe0nn3xyx7Zu98J3+6lpTC0iPNVyenYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIL72VFqyZIlpe3Lly8vbd+/f3/HNs6j91fXnt32JtvjtvdOWjbP9nbbbxePc5stE0BV0xnG3yfpimOW3SxpR0ScKWlH8RrAAOsa9ojYJenjYxavlrS5eL5Z0lX1lgWgbr0esy+IiDFJiogx2/M7vdH2kKShHrcDoCaNf0EXEcOShiVuhAHa1Oupt0O2F0pS8TheX0kAmtBr2LdJWlc8Xyfp8XrKAdCUrsN42w9KuljSSbZHJd0q6XZJj9i+XtJ7kq5uski056KLLqq0/ocfflhTJaiqa9gjYm2HpktqrgVAg7hcFkiCsANJEHYgCcIOJEHYgSS4xRWlzjvvvErrb9y4saZKUBU9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwZTNyV144YWl7U899VRp+4EDB0rbV6xY0bHtq6++Kl0XvWHKZiA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgvvZk7v00ktL2+fNm1fa/swzz5S2cy59cNCzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdP7vzzzy9t7/Z7B1u2bKmzHDSoa89ue5Ptcdt7Jy27zfYHtvcUf6uaLRNAVdMZxt8n6Yoplv8lIpYWf3+vtywAdesa9ojYJenjPtQCoEFVvqBbb/vVYpg/t9ObbA/ZHrE9UmFbACrqNex3SzpD0lJJY5Lu6PTGiBiOiOURsbzHbQGoQU9hj4hDEXE4Io5IukfSBfWWBaBuPYXd9sJJL9dI2tvpvQAGQ9ffjbf9oKSLJZ0k6ZCkW4vXSyWFpAOSboiIsa4b43fj++6UU04pbd+zZ09p+yeffFLafs4553zfktCwTr8b3/WimohYO8XieytXBKCvuFwWSIKwA0kQdiAJwg4kQdiBJLjFdYa77rrrStvnz59f2v7000/XWA3aRM8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwnn2GW7x4caX1u93iih8OenYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSILz7DPclVdeWWn9J554oqZK0DZ6diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsM8DKlSs7tnWbshl5dO3ZbZ9m+3nb+2y/bntDsXye7e223y4e5zZfLoBeTWcY/42k30XEOZIulHSj7Z9JulnSjog4U9KO4jWAAdU17BExFhEvF88/k7RP0iJJqyVtLt62WdJVDdUIoAbf65jd9hJJyyS9IGlBRIxJE/8h2J5y0jDbQ5KGKtYJoKJph932CZIelXRTRHxqe1rrRcSwpOHiM6KXIgFUN61Tb7ZnayLoD0TEY8XiQ7YXFu0LJY03UyKAOnTt2T3Rhd8raV9E3DmpaZukdZJuLx4fb6RCdLVmzZqObbNmzSpd95VXXilt37VrV081YfBMZxi/QtJvJL1me0+x7BZNhPwR29dLek/S1Y1UCKAWXcMeEf+U1OkA/ZJ6ywHQFC6XBZIg7EAShB1IgrADSRB2IAlucf0BOP7440vbV61a1fNnb9mypbT98OHDPX82Bgs9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k4Yj+/XgMv1TTm9mzZ5e279y5s2Pb+Hj5b4pce+21pe1ffvllaTsGT0RMeZcqPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMF5dmCG4Tw7kBxhB5Ig7EAShB1IgrADSRB2IAnCDiTRNey2T7P9vO19tl+3vaFYfpvtD2zvKf56//FyAI3relGN7YWSFkbEy7ZPlPSSpKsk/VrS5xHx52lvjItqgMZ1uqhmOvOzj0kaK55/ZnufpEX1lgegad/rmN32EknLJL1QLFpv+1Xbm2zP7bDOkO0R2yPVSgVQxbSvjbd9gqSdkv4UEY/ZXiDpI0kh6Y+aGOr/tstnMIwHGtZpGD+tsNueLelJSc9GxJ1TtC+R9GREnNvlcwg70LCeb4SxbUn3Sto3OejFF3dHrZG0t2qRAJoznW/jV0r6h6TXJB0pFt8iaa2kpZoYxh+QdEPxZV7ZZ9GzAw2rNIyvC2EHmsf97EByhB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSS6/uBkzT6S9J9Jr08qlg2iQa1tUOuSqK1Xdda2uFNDX+9n/87G7ZGIWN5aASUGtbZBrUuitl71qzaG8UAShB1Iou2wD7e8/TKDWtug1iVRW6/6Ulurx+wA+qftnh1AnxB2IIlWwm77Cttv2X7H9s1t1NCJ7QO2XyumoW51frpiDr1x23snLZtne7vtt4vHKefYa6m2gZjGu2Sa8Vb3XdvTn/f9mN32LEn7JV0maVTSbklrI+KNvhbSge0DkpZHROsXYNj+haTPJf316NRatjdK+jgibi/+o5wbEb8fkNpu0/ecxruh2jpNM36dWtx3dU5/3os2evYLJL0TEe9GxNeSHpK0uoU6Bl5E7JL08TGLV0vaXDzfrIl/LH3XobaBEBFjEfFy8fwzSUenGW9135XU1RdthH2RpPcnvR7VYM33HpKes/2S7aG2i5nCgqPTbBWP81uu51hdp/Hup2OmGR+YfdfL9OdVtRH2qaamGaTzfysi4ueSfiXpxmK4ium5W9IZmpgDcEzSHW0WU0wz/qikmyLi0zZrmWyKuvqy39oI+6ik0ya9PlXSwRbqmFJEHCwexyVt1cRhxyA5dHQG3eJxvOV6/i8iDkXE4Yg4IuketbjvimnGH5X0QEQ8Vixufd9NVVe/9lsbYd8t6Uzbp9s+TtI1kra1UMd32J5TfHEi23MkXa7Bm4p6m6R1xfN1kh5vsZZvGZRpvDtNM66W913r059HRN//JK3SxDfy/5b0hzZq6FDXTyX9q/h7ve3aJD2oiWHdfzUxIrpe0k8k7ZD0dvE4b4Bqu18TU3u/qolgLWyptpWaODR8VdKe4m9V2/uupK6+7DculwWS4Ao6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjif1f9vw1I/2nmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = X_test[2], Y_test[2]\n",
    "print (\"This is an image of Number\", y)\n",
    "pixels = x.reshape((28,28))\n",
    "plt.imshow(pixels,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj3CKI4vbB-i"
   },
   "source": [
    "Check if the model is loaded correctly. The test accuracy may be 97.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "6s9CcklpbB-i",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.976\n"
     ]
    }
   ],
   "source": [
    "nTest = 1000\n",
    "Y_pred = np.zeros(nTest)\n",
    "for i in range(nTest):\n",
    "    x, y = X_test[i], Y_test[i]\n",
    "    #print(x, y)\n",
    "    Y_pred[i] = clf.predict(x[None,:])\n",
    "acc = np.sum(Y_pred == Y_test[:nTest])*1.0/nTest\n",
    "print (\"Test accuracy is\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nawyjY8qbB-i"
   },
   "source": [
    "(d) **[2 points]** Please generate an adversarial example and check the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "GQNOMrn1bB-i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an adversarial image of Number 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25d8a60a790>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Output pixels: the adversarial example. \n",
    "### YOUR CODE HERE\n",
    "x, y = X_test[0], Y_test[0]\n",
    "x_adv = clf.attack(x[None,:], y)\n",
    "print (\"This is an adversarial image of Number\", y)\n",
    "pixels = x_adv.reshape((28,28))\n",
    "### END YOUR CODE\n",
    "plt.imshow(pixels,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXUizF62bB-j"
   },
   "source": [
    "(e) **[3 points]** Try the adversarial attack and test the accuracy of using adversarial examples.\n",
    "\n",
    "You can get a test accuracy of using adversarial examples after running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "aPWZ1gNFbB-j",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of adversarial examples is 0.286\n"
     ]
    }
   ],
   "source": [
    "### Output acc: the adversarial accuracy. \n",
    "### YOUR CODE HERE\n",
    "nTest = 1000\n",
    "Y_pred = np.zeros(nTest)\n",
    "for i in range(nTest):\n",
    "    x, y = X_test[i], Y_test[i]\n",
    "    x_adv = clf.attack(x[None,:], y)\n",
    "    Y_pred[i] = clf.predict(x_adv)\n",
    "acc = np.sum(Y_pred == Y_test[:nTest])*1.0/nTest\n",
    "\n",
    "### END YOUR CODE\n",
    "print (\"Test accuracy of adversarial examples is\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
